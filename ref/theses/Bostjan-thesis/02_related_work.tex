%!TEX root = thesis.tex

\chapter{Related Work}
\label{chap:related_work}

%TODO\footnote{This chapter is not ready yet.}

In this chapter, we review the related work in two research areas: activity recognition, which includes activity recognition in computer vision and sensor-based activity recognition; and anomalous and suspicious behavior detection based on pattern analysis, transaction analysis, and plan recognition.

%
%==========================================================================================
%
\section{Activity Recognition}

\index{activity recognition}
Activity recognition is the process whereby an agent's behavior and its situated environment are monitored and analyzed to infer the undergoing activities~\citep{Chen2012}. Researchers from different application domains have investigated activity recognition for the past decade by developing a diversity of approaches. We broadly classify activity recognition in categories based on monitoring facilities, which are responsible for capturing contextual information for activity recognition systems to infer agent's activity. There are currently two main activity recognition approaches: vision-based and sensor-based activity recognition.

%---------------------------------------------------------------------------
%
\subsection{Vision-Based Activity Recognition}

Tracking and understanding the behavior of agents through videos has been a research focus for a long period due to its important role in areas, such as human-computer interaction and surveillance. In vision-based activity recognition, researchers have attempted a wide variety of methods, such as optical flow, Kalman filtering, hidden Markov models, and conditional random fields, under different modalities such as single camera, stereo, and infrared. In addition, researchers have considered multiple aspects on this topic, including single agent tracking, multiple-agent tracking, activity recognition, compound-activity recognition, and finally recognition of multi-agent interactions.

The activity recognition process is typical composed of four steps, namely agent detection, agent tracking, activity recognition and then a high-level activity evaluation. \cite{Chen2011} in their review conclude that while 
%considerable work has been undertaken and 
significant progress has been made, vision-based activity recognition approaches suffer from issues related to scalability and re-usability due to complexity of real world settings; that is, high variability of activities and environment. In addition, cameras are in some communities perceived as invasive, which may prevent this approach from large-scale uptake in some applications, such as home environments.


%Muncaster~\cite{Muncaster07model} presented a framework for hierarchical activity recognition, where a moving object was first extracted from a video stream and then a dynamic Bayesian network was applied to model the activities at different granularities. In the test scenario the system was able to distinguish a person entering, leaving or passing the shop. 

%---------------------------------------------------------------------------
%
\subsection{Sensor-Based Activity Recognition}

\index{sensors}
Sensor-based activity recognition exploits a wide range of sensors, including accelerometers, RFID tags, audio and motion detectors, to name but a few, to monitor an agent behavior along with its environment. These sensors differ in purpose, technical infrastructure, output signals, and underpinning theoretical principles. However, they can be classified in two main categories in terms they are deployed in activity monitoring applications~\citep{Chen2012}: wearable sensors and embedded sensors.


%Many researchers have contributed to automated activity recognition. Typically, an automated system for daily-living analysis has three main components: (i) sensing hardware that gathers relevant information about activities (for example, a video camera, a marker-based motion capture, accelerometers, gyroscopes, a localization system); (ii) low-level activity recognition that discriminates sensed postures (for example, walking, sitting, lying etc); and (iii) high-level activity analysis or recognition of activity patterns or daily behavior (for example, preparing a meal, shopping, daily dynamics). Choudhury et al.~\cite{Choudhury06towards} reviewed several approaches identifying rich sensors (camera, microphone), personalized sensors (attached to a person -- accelerometers, location tags) and dense sensors (attached to objects -- RFID) as the most common sensing component, while methods used in the second and the third components can be divided into generative (na{\"i}ve Bayesian model, hidden Markov models, dynamic Bayesian networks) and discriminative (SVMs, logistic regression, conditional random fields). 

\subsubsection{Wearable Sensors}

\index{sensors!wearable}
Wearable sensors are positioned directly or indirectly on the body of an agent to generate signals while the agent performs activities. When the observed entity is human, wearable sensors can be embedded into clothes, eyeglasses, waists, shoes, mobile device, or positioned directly on the body. They can be used to collect information, such as position, velocity and acceleration of various body parts, pulse, and skin temperature. In the following, we summarize the inertial sensors (for example, accelerometers, gyroscopes, magnetometers), vital sign sensors (heart rate, temperature), and visual markers.


Accelerometer sensors are probably the most frequently exploited wearable sensors, since they are both inexpensive and effective. The first generation of methods was based on a tri-axial accelerometer with threshold algorithms \citep{Kangas2008}. \cite{Bourke} introduced a threshold algorithm to distinguish between normal activities (sitting down and standing up, lying down and standing up, getting in and out of a car seat, walking etc.) and falls. The ability to discriminate was achieved using a bi-axial gyroscope mounted on the torso, measuring pitch and roll angular velocities. They applied a threshold algorithm to the peaks in the angular velocity signal, angular acceleration and torso angle change.
%
The second generation of methods is able to classify activities with machine-learning methods, such as decision trees, SVM, kNN, and na{\"i}ve Bayes. 
\cite{Hunyhn07scalable} presented an approach for recognizing daily activities. The movement was sensed by three body-worn accelerometers, while the recognition of 15 low-level and three high-level activities was performed using four approaches: k-means clustering, SVM, nearest neighbor classifier, and hidden Markov models. In the experimental setting the system achieved an accuracy of $69 - 80\%$ for low-level (for example, sit, eat, walk) and $83 - 92\%$ for high-level (preparing for work, shopping, housework) activities. 
%
\cite{Tapia2007} presented a real-time algorithm for automatic recognition of not only physical activities, but also, in some cases, their intensities, using five wireless accelerometers and a wireless heart rate monitor. The accelerometers were placed at shoulder, wrist, hip, upper part of the thigh and ankle. The features, for example, FFT peaks, variance, energy, correlation coefficients, were extracted from time and frequency domains using a predefined window size on the signal. The classification of activity was done with C4.5 and na{\"i}ve Bayes classifiers into three groups: postures (for example, standing, sitting), activities (for example, walking, cycling) and other activities (for example, running, using stairs). For these three classes they obtained the recognition accuracy of 94.6\% using subject-dependent training and 56.3\% using subject-independent training. 
%
\cite{Kwapisz2011} used an accelerometer placed on the thigh and compared the results of three classification methods on dynamic activities such as walking, running and jogging. 
%Ravi et al. [21] used an accelerometer on a mobile phone and tested their single-layer approach with five classification methods. The results showed that when the same person's data was used for training and testing, the accuracy was 90\%, but when a different person's data was used for the testing, the accuracy dropped to 65\%. 
%
\cite{Banos2013} proposed a hierarchical-weighted classification that combines the majority voting and weighted hierarchical aggregation: at the first level each sensor makes decision about the recognized activity using binary classifiers, while at the next level, weighted majority vote scheme aggregates the decision in order to make the final decision. 
 
\cite{Qian2004} introduced a gesture-driven interactive dance system capable of real-time feedback. They used 41 markers on the body recorded by eight cameras with the frame rate of 120 Hz to construct a human body model. The model was used to extract features such as torso orientation, angles between adjacent body parts etc., which was used to represent different gestures. Each gesture was statistically modeled with a Gaussian random vector defined as the statistical distribution of the features for that gesture. To recognize a new pose, the likelihood of its feature vector given the vector of each known gesture was computed. The new pose was classified as the gesture for which this likelihood was the largest. Experimental results with two dancers performing 21 different gestures achieved gesture recognition rate of 99.3\%. \cite{Sukthankar2005} presented a system that reconstructs the usersâ€™ posture and recognizes pre-defined behaviors. The data were captured with 43 body markers and 12 cameras with the sampling rate of 120 Hz. They constructed a human body model from the raw marker coordinates, and computed features, for example the angles between body parts, limb lengths, range of motion etc. from the model. Learning was performed using SVM. The method achieved 76.9\% accuracy in detecting walking, running, sneaking, being wounded, probing, crouching, and rising. Behavior was defined as a sequence of elementary activities and was modeled with hidden Markov models. The authors defined a number of behavior models and classified a new sequence of activities into the model that fit it best.

Our work follows the second-generation acceleration-based activity recognition, but it demonstrates an approach based on wearable location sensors, where considerable amount of noise is present. In contrast to related work, it performs activity recognition in pipeline; that is, noise removal, activity recognition, removal of spurious activity transitions, and recognition of complex activities. Compared to work by \cite{Sukthankar2005} and \cite{Qian2004}, our work deals with two orders of magnitude less accurate location system and only four location tags.

Activity recognition based on wearable sensors suffers from some limitations~\citep{Chen2012}; that is, most of the sensors need to run constantly and be operated hands-free. Practical issues involve the user acceptability and ability to wear the sensors, while technical issues include size, battery life and ease of use. Moreover, wearable sensors may not be suitable for monitoring activities that include interactions with the environment. As a result, it is often advantageous to combine wearable sensors with embedded sensors, which are described bellow.



\subsubsection{Embedded Sensors}

\index{sensors!embedded}
\index{sensors!dense}
Embedded sensors, sometimes referred to as dense sensors, are attached to objects and activities are monitored by detecting object-agent interactions. Using dense sensing, a large number of usually low-cost, miniaturized sensors are deployed in a range of objects and locations within in an environment. This approach is based on the assumption that activities are characterized by the objects that are manipulated during their performance; that is, activities can be recognized from sensor data that monitor agent interactions with the objects in the environment~\citep{Chen2012}. 

\index{ambient assisted living, AAL}
\index{activities of daily living, ADL}
Activity recognition based on embedded sensors has been widely adopted in AAL via smart home paradigm to monitor an inhabitant's movements and environmental events, providing just-in-time context-aware ADL assistance. For example,
\cite{Storf} studied recognition of ADLs from sensors embedded in the environment. They introduced a multiagent approach that uses an event-driven activity recognition language to compose atomic activities into high-level activities. The authors report accuracy of higher than 80\%.  In a similar setting \cite{Cook2012} applied hidden Markov models for recognition of ADLs and varied the number of sensors used for recognition. The achieved accuracy ranged between 80\% and 90\%, and dropped below 75\% when significant number of sensors was removed.

Different types of sensors and modalities have been in different combinations for activity recognition, and it is impossible to claim that one sensor combination is superior. The suitability and performance are tightly related to the type of activities being assessed and the characteristics of the concrete applications.

%\subsection{Activity Recognition}

%\subsection{Mutliagent Activity Recognition}


% Activities of Daily Living (ADL) is a term used in medicine and nursing, especially in the care of the elderly. It describes the things we normally do during a day. Manual assessment by an observer or self-reporting helps practitioners determine how independent persons are and what skills they can accomplish on their own, for example, driving, cleaning, cooking, shopping, bathing, dressing, feeding, and toileting. The evaluator scores various activities in each category to determine the person's skill. The score is compared to the score of the previous visit, which leads to a decision as to whether supervision or assistance is needed [2]. 

% 


% %
%
% %
% 
% %


% In this paper the system uses a localization system (in other publications, accelerometers are more often used) with body-worn wireless tags (described in Section \ref{sec:experiments}), while low-level activity recognition is performed with a Random Forest classifier. These two modules were developed within the Confidence system~\cite{Confidence08}. The focus of this paper is on the third component, the analysis of daily patterns that aims to detect changes in behavior that indicate an early discovery of a potential health problem, for example, a person visits a toilet unusually often. In contrast to related work, which mainly dealt with a description of high-level activities, our method focuses on the dynamics of activities, and in addition on Markov models, for exploring the relations between spatial information and activities. %The method detects anomalous behavior regardless of the cause. 



%\subsection{Overview}

% \subsubsection{Body-Worn Sensors}
% \subsubsection{Visual Markers}
% \subsubsection{Video and Other Sensors}



% \subsection{Complex Activity Recognition}
% \subsubsection{Logical Approaches}
% \subsubsection{Probabilistic Approaches}


%
%==========================================================================================
%
\section{Anomalous and Suspicious Behavior Detection}

 There are two approaches to detecting deviant behavior~\citep{Avrahami-Zilberbrand2009}: \emph{suspicious} and \emph{anomalous} behavior detection. The first approach assumes a behavior library that encodes \emph{negative behavior}, and thus recognizing observed behavior corresponds to identifying a match in the library. The second approach uses the behavior library in an inverse fashion, meaning that the library encodes only \emph{positive behavior}. When an observed behavior cannot be matched against the library it is considered as anomalous. Several approaches have been proposed to tackle the problem either way. We broadly classify anomalous and suspicious behavior detection in three categories: pattern analysis, transaction analysis, and plan recognition.

\subsection{Pattern Analysis}
Anomalous and suspicious behavior detection from patterns is usually based on visual modalities, such as camera. Trajectories of moving objects have been used to infer anomalous agent paths~\citep{Zhang2004,Vaswani}, although image-plan trajectory itself is sensitive to translations, rotations and scale changes. \citet{Zhang2007} proposed a system for a visual human motion analysis from a video sequence, which recognizes unusual behavior based on  walking trajectories, namely treading tracks. Two types of line shapes were studied: the closed curve and the spiral line. If preson's treading track takes on one of these shapes, this person is wandering around and is, therefore, suspicious. \citet{Lin2008} described a video surveillance system based on color features, distance features, and a count feature, where evolutionary techniques are used to measure observation similarity. The system tracks each person and classifies their behavior by analyzing their trajectory patterns. This is performed with a hybrid genetic algorithm that uses a Gaussian synapse. Another approach includes behavior patterns based on visual features, for example, \cite{Arsic} introduced an approach to visual surveillance of public transportation systems. The system extracts a set of visual low-level features in different parts of the image, and performs a classification with SVMs to detect aggressive, cheerful, intoxicated, nervous, neutral, and tired behavior.



\subsection{Transaction Analysis}
\label{related:behavior:transaction}

\index{intrusion detection}
Transaction analysis assumes discrete states/transations in contrast to pattern analysis, which is based on continuous observations. A major research area is intrusion detection (ID) that aims detecting attacks against information systems in general. There are two types of ID systems: signature based and anomaly based. \cite{Helman1993} proposed an intrusion detection system that provides a rating for computer activities, demonstrating frequentist estimator \index{frequentist estimator} and matching rules. \cite{Esponda2004} analyzed trade-offs between positive and negative activity patterns in the library and presented an approach based on partially matching rules. These approaches similarly address the problem of how to decide whether a user's activity is suspicious, but differ significantly in the approach to matching and assessing the  behavior. A comprehensive review of ID approaches was recently published by~\cite{Gyanchandani2012}. \citet{Quah2008} presented an approach to online-banking fraud detection based on persons' spending behaviors. Their approach makes use of a self-organization map to learn persons' spending patterns, while neural networks filter any unusual events and analyze the person behavior in order to detect fraud. In addition, \citet{Alexandre1997}  proposed a system based on the keyboard signature behavior recognition, which is more difficult to copy or fake than a fingerprint or a smart card. The presented technique implements a neural network, which is evaluated in terms of efficiency and performance. 

Our work leverages ideas by \cite{Helman1993} and \citet{Esponda2004} to establish a formal detection framework based on behavior patterns and analyze detection errors. On this basis, we extend the framework to formally address repeated behavior detection and specify conditions any reasonable detector should satisfy.

\index{ambient assisted living, AAL}
Furthermore, AAL applications based on wearable sensors also fit to transaction analysis, since sensing is typically event based. \cite{Lymberopoulos} proposed a system for automatic extraction of the users' spatio-temporal patterns from the sensor network deployed inside their home. The proposed method, based on location, time and duration, was able to extract frequent patterns using the Apriori algorithm and to encode the most frequent patterns in the form of a Markov chain, while our work uses the location and the activity performed by the user to build a model of normal behavior and detect anomalous behavior patterns. 
%We focus to related research within recognition from multiple events emphasasing  probabilistic models and plan recognition. 
\index{hidden Markov models} Another area of related work includes hidden Markov models (HMMs)~\citep{Rabiner1989} that are widely used in traditional activity recognition for modeling a sequence of actions.  \cite{Brand} introduced coupled HMMs as an extension with multiple hidden interacting chains that are able to model interactive behavior. 
%Moreover, layered HMMs~\cite{Oliver2004} and hierarchical HMMs~\cite{Fine1998} can handle activities that have hierarchical structure, for example, activity recognition from trajectories \cite{Nguyen2005}. 
\cite{Duong2005} focused on the duration of activities and introduced switching hidden semi-Markov models that provide probabilistic constraints over the duration of plans, and applied them to the detection of anomalies in the activities of daily living. 
%Vaswani et al.~\cite{Vaswani} introduced Continuous State HMMs for modeling trajectories in order to detect anomalous activities. 
~\cite{Monekosso} used embedded sensors and also addressed the problem of anomalous behavior detection. The output of the sensors was directly used to train a HMM model based on normal observations. If the likelihood that a new observation was generated by the trained model was low, the behavior was considered abnormal. Our work first recognizes the user's activities from sensor data and then combines them with spatial information. Compared to HMMs, it does not require an estimation of the parameters in the learning phase.
Although widely used, HMMs may become inadequate when actions are more complex or have long-term temporal dependencies~\citep{KollerFriedman2009}. 

\cite{Lee04daily} proposed a fuzzy-association analysis of an individual's daily patterns based on an infrared location sensor and activity sensor groups (for example, sleeping, eating, leisure sensor group). They defined two fuzzy membership functions: start time (for example, dawn, morning) and duration (for example, short, medium), and transformed a sequence of activities using these two functions to categorical attributes. Afterwards, the Apriori algorithm was applied to the dataset, searching for activity patterns. The authors suggest that the behavioral pattern changes indicate that the person is not well. 
%
\cite{Lymberopoulos} proposed a system for automatically extracting person spatio-temporal patterns from a home-deployed sensor network. The proposed method, based on location, time, and duration, was able to extract patterns using the Apriori algorithm and to encode the most frequent ones in a Markov chain. 

In contrast to related work, we propose a presentation that encodes activity dynamics; that is, it explores the relations between spatial information and activities to capature behavior dynamics in a specific time period. Our work first recognizes the user's activities from sensor data and then combines them with spatial information. %Compared to HMMs, it does not require an estimation of the parameters in the learning phase.


\subsection{Plan Recognition}

\index{plan recognition}
Plan recognition focuses on a mechanism for recognizing the unobservable state of an agent, given observations of its interaction with its environment~\citep{Avrahami-Zilberbrand2009}. Most existing investigations assume discrete observations in a form of activities. To perform anomalous and suspicious behavior detection, plan recognition algorithms may use a hybrid approach: a symbolic plan recognizer is used to filter consistent hypotheses, passing them to an evaluation engine, which focuses on ranking. 

\index{utility-based plan recognition!UPR}
\cite{Avrahami-Zilberbrand2007} presented Utility-based Plan Recognition (UPR) that introduces utility to the observer in selecting the recognition hypotheses. The main strength of UPR is that it can incorporate an observer's bias to events with a low likelihood, for example, the a-priori probability for planting a bomb is very low, but detecting it has a high expected utility. We further discuss this approach in Section~\ref{sec:UPR}.  \cite{Geib2009} presented PHATT, a probabilistic approach based on tree grammars able to cope with interleaved goals, partially ordered plans, and failed observed actions. \cite{Sukthankar-AAAI2008} addressed plan recognition for multiagent teams, where plans were ordered by linear accumulation of observed actions consistent with the plan.

Our work leverages UPR approach to perform repeated behavior detection. The notion of utility, which is assigned to each plan step by the observer, is extended with the notion of utility function that generalizes utility-based plan recognition with arbitrary utility functions. This allows to assign utility to repeated plan steps according to agent past behavior. 


%
%==========================================================================================
%

